# -*- coding: utf-8 -*-
"""FinancialEconometrics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cCw0b921wWDD-qwUvtqOelfzSevz16_H

Python of how to simulate and check for covariance stationarity in a time series using the Augmented Dickey-Fuller (ADF) test:
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.stattools import adfuller

# Simulate a stationary time series
np.random.seed(123)
n = 1000
x = np.random.randn(n)

# Check for stationarity using the ADF test
result = adfuller(x)
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print('Critical Values:')
for key, value in result[4].items():
    print('\t%s: %.3f' % (key, value))
    
# Simulate a non-stationary time series
np.random.seed(123)
n = 1000
x = np.cumsum(np.random.randn(n))

# Check for stationarity using the ADF test
result = adfuller(x)
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print('Critical Values:')
for key, value in result[4].items():
    print('\t%s: %.3f' % (key, value))

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate a time series with autocorrelation
np.random.seed(123)
n = 1000
x = np.cumsum(np.random.randn(n))

# Calculate autocovariance and autocorrelation functions
acf = pd.Series(np.correlate(x, x, mode='full')[-n:], index=range(n))
acf /= acf[0]  # normalize by the variance
acf.plot(kind='bar')
plt.title('Autocorrelation Function')
plt.xlabel('Lag')
plt.show()

# Calculate the autocovariance function directly
cov = np.zeros(n)
for k in range(n):
    cov[k] = np.cov(x[:n-k], x[k:n])[0,1]
acf2 = pd.Series(cov / cov[0], index=range(n))
acf2.plot(kind='bar')
plt.title('Autocovariance Function')
plt.xlabel('Lag')
plt.show()

"""White Noise"""

import numpy as np
import matplotlib.pyplot as plt

# Generate a white noise time series with length 100
white_noise = np.random.normal(loc=0, scale=1, size=100)

# Plot the time series
plt.plot(white_noise)
plt.title('White Noise Time Series')
plt.xlabel('Time')
plt.ylabel('Value')
plt.show()

"""ARMA Process"""

import pandas as pd
import numpy as np
import statsmodels.api as sm
from statsmodels.tsa.arima.model import ARIMA


# Generate some sample data
np.random.seed(123)
n = 100
eps = np.random.normal(size=n)
y = np.zeros(n)
for i in range(2, n):
    y[i] = 0.7*y[i-1] - 0.2*y[i-2] + eps[i] - 0.3*eps[i-1]

# Fit an ARMA(2,1) model to the data

model = ARIMA(y, order=(2, 0, 1))

results = model.fit()

# Print the model summary
print(results.summary())

# Make predictions for the next 10 time steps
preds = results.forecast(steps=10)

# Plot the original time series and the predicted values
import matplotlib.pyplot as plt
plt.plot(y, label='Observed')
plt.plot(range(n, n+10), preds, label='Predicted')
plt.legend()
plt.show()

"""Sample quantiles"""

import numpy as np

# Generate a sample dataset
data = np.random.normal(loc=0, scale=1, size=100)

# Compute the median (50th percentile)
median = np.median(data)

# Compute the first and third quartiles (25th and 75th percentiles)
q1 = np.percentile(data, 25)
q3 = np.percentile(data, 75)

# Compute the interquartile range (IQR)
iqr = q3 - q1

# Print the results
print("Median:", median)
print("First quartile:", q1)
print("Third quartile:", q3)
print("Interquartile range:", iqr)

import numpy as np

returns = [-0.020, 0.015, 0.012, 0.005, -0.009, -0.008, -0.013, 0.021, -0.011, 0.010]

# Compute the median (50th percentile)
median = np.median(returns)

# Compute the first and third quartiles (25th and 75th percentiles)
q1 = np.percentile(returns, 25)
q3 = np.percentile(returns, 75)

# Compute the interquartile range (IQR)
iqr = q3 - q1

# Print the results
print("Median:", median)
print("First quartile:", q1)
print("Third quartile:", q3)
print("Interquartile range:", iqr)

"""**Jarque-Bera test** in Python using the scipy.stats module:

The Jarque-Bera (JB) test is a statistical test used to determine if a given dataset follows a normal distribution. The test is based on the skewness and kurtosis of the dataset, and is named after its inventors, Carlos Jarque and Anil K. Bera.

The null hypothesis of the JB test is that the dataset is normally distributed. The test statistic is calculated as:

JB = n/6 * (S^2 + 1/4 * (K-3)^2)

where n is the sample size, S is the sample skewness, and K is the sample kurtosis.

Under the null hypothesis, the test statistic JB follows a chi-squared distribution with 2 degrees of freedom. If the p-value associated with the test statistic is less than a pre-specified significance level (e.g., 0.05), then we reject the null hypothesis and conclude that the dataset is not normally distributed.

One limitation of the JB test is that it is sensitive to sample size. For small sample sizes, the test may have low power to detect departures from normality, while for large sample sizes, the test may be too sensitive and reject the null hypothesis even when the deviations from normality are minor.

In practice, the JB test is often used in conjunction with other diagnostic tests and visualizations to assess the normality of a dataset.
"""

from scipy.stats import jarque_bera, norm
import numpy as np

# Generate a random sample from a normal distribution
np.random.seed(123)
sample = np.random.normal(0, 1, 100)

# Compute the Jarque-Bera test statistic and p-value
jb, p_value = jarque_bera(sample)

# Compute the critical value for a significance level of 0.05
crit_val = norm.ppf(0.95)

# Print the results
print("Jarque-Bera test statistic:", jb)
print("p-value:", p_value)
print("Critical value:", crit_val)

if p_value < 0.05:
    print("Reject null hypothesis: Sample is not normally distributed")
else:
    print("Fail to reject null hypothesis: Sample is normally distributed")

"""How to perform the Ljung-Box test and the Durbin-Watson test on a time series using Python and the statsmodels library"""

import pandas as pd
import numpy as np
import statsmodels.api as sm

# Load time series data
data = pd.read_csv('data.csv', index_col=0, parse_dates=True)

# Calculate autocorrelation coefficients up to lag 10
lags = 10
acf, q, p = sm.tsa.acf(data, nlags=lags, qstat=True)

# Perform Ljung-Box test
lb_pvalue = sm.stats.acorr_ljungbox(data, lags=[lags])[1][-1]
if lb_pvalue < 0.05:
    print("Reject null hypothesis of no autocorrelation")
else:
    print("Cannot reject null hypothesis of no autocorrelation")

# Perform Durbin-Watson test
dw_pvalue = sm.stats.durbin_watson(data)
if dw_pvalue < 0.05:
    print("Reject null hypothesis of no first-order autocorrelation")
else:
    print("Cannot reject null hypothesis of no first-order autocorrelation")

import numpy as np
import statsmodels.api as sm

# Generate example time series data
np.random.seed(123)
data = np.random.normal(0, 1, 1000)

# Calculate autocorrelation coefficients up to lag 10
lags = 10
acf, q, p = sm.tsa.acf(data, nlags=lags, qstat=True)

# Perform Ljung-Box test
lb_pvalue = sm.stats.acorr_ljungbox(data, lags=[lags])[1][-1]
if lb_pvalue < 0.05:
    print("Reject null hypothesis of no autocorrelation")
else:
    print("Cannot reject null hypothesis of no autocorrelation")

# Perform Durbin-Watson test
dw_pvalue = sm.stats.durbin_watson(data)
if dw_pvalue < 0.05:
    print("Reject null hypothesis of no first-order autocorrelation")
else:
    print("Cannot reject null hypothesis of no first-order autocorrelation")

"""The Durbin-Watson and Ljung-Box tests are commonly used to test for autocorrelation in time series data. However, if the data is non-normal or has outliers, these tests may not be reliable. In such cases, a robust test for autocorrelation may be more appropriate.

One robust test for autocorrelation is the Breusch-Godfrey test. This test is an extension of the Ljung-Box test, but it is based on a regression model. The Breusch-Godfrey test involves regressing the residuals from an initial regression model on the lagged residuals, and testing for the significance of the coefficients on the lagged residuals.

Here's an example of how to perform the Breusch-Godfrey test using the statsmodels library in Python:
"""

import numpy as np
import statsmodels.api as sm

# Generate example time series data
np.random.seed(123)
data = np.random.normal(0, 1, 1000)

# Fit initial regression model
X = np.arange(len(data))
X = sm.add_constant(X)
model = sm.OLS(data, X).fit()
residuals = model.resid

# Calculate autocorrelation coefficients up to lag 10
lags = 10
acf = sm.tsa.acf(residuals, nlags=lags, qstat=False)

# Perform Breusch-Godfrey test
bg_pvalue = sm.stats.diagnostic.acorr_breusch_godfrey(residuals, X, nlags=lags)[1]
if bg_pvalue < 0.05:
    print("Reject null hypothesis of no autocorrelation")
else:
    print("Cannot reject null hypothesis of no autocorrelation")

"""Joint tests on many autocorrelations can be used to determine if there is a significant departure from the null hypothesis of no autocorrelation for a range of lags.

One example of such a test is the Box-Pierce test, which is similar to the Ljung-Box test but with a less stringent asymptotic distribution. The Box-Pierce test statistic is defined as:

**Q = n * (R1^2 + R2^2 + ... + Rp^2)**

where n is the sample size, Rk is the sample autocorrelation at lag k, and p is the number of lags being tested. Under the null hypothesis of no autocorrelation, the test statistic Q follows a chi-squared distribution with p degrees of freedom.

In Python, you can perform the Box-Pierce test using the acorr_ljungbox function from the statsmodels module. Here's an example:
"""

import numpy as np
import statsmodels.api as sm

# Generate example time series data
np.random.seed(123)
data = np.random.normal(0, 1, 1000)

# Calculate autocorrelation coefficients up to lag 10
lags = 10
acf, q, p = sm.tsa.acf(data, nlags=lags, qstat=True)

# Perform Box-Pierce test
bp_pvalue = sm.stats.acorr_ljungbox(data, lags=[lags], boxpierce=True)[1][-1]
if bp_pvalue < 0.05:
    print("Reject null hypothesis of no autocorrelation")
else:
    print("Cannot reject null hypothesis of no autocorrelation")



"""**Modelling covariance matrices**

Modelling covariance matrices is an important task in multivariate analysis, especially in finance, where investors are interested in understanding the interdependence and risk of portfolios composed of multiple assets. There are several techniques for modelling covariance matrices, including:

1. Historical covariance: This approach estimates the covariance matrix based on historical returns of the assets. While this method is simple and easy to implement, it assumes that the future will be similar to the past, which may not always hold true.

2. Constant correlation: This approach assumes a constant correlation between the assets. While this simplifies the model, it may not capture the time-varying nature of the correlation.

3. Factor models: Factor models decompose the covariance matrix into a systematic component and an idiosyncratic component. The systematic component is usually modeled as a function of a few common factors, while the idiosyncratic component is modeled using univariate variance models. This approach can capture the interdependence among assets and provide a more accurate estimate of the covariance matrix.

4. Dynamic conditional correlation (DCC) models: DCC models estimate the correlation matrix as a function of the past returns and past correlations. This approach can capture the time-varying nature of the correlation and provide a more accurate estimate of the covariance matrix.

5. Multivariate GARCH models: These models extend the univariate GARCH models to multiple assets and estimate the covariance matrix as a function of the past returns and past covariance. Multivariate GARCH models can capture the interdependence among assets and provide a more accurate estimate of the covariance matrix.

Overall, modelling covariance matrices is a complex task, and the choice of the technique depends on the underlying assumptions, data availability, and the objective of the analysis.

In this example, we assume that we have historical returns of three assets and store them in a numpy array. We then use the np.cov() function to estimate the covariance matrix, which is stored in the cov_matrix variable. The resulting matrix represents the covariance between each pair of assets.

However, it's important to keep in mind that historical covariance may not always accurately reflect the true covariance structure in the future, as market conditions can change. Therefore, this approach should be used with caution and complemented with other techniques for modelling covariance matrices.
"""

import numpy as np

# define the historical returns of three assets
returns = np.array([[0.05, 0.02, 0.01], [0.02, 0.04, 0.03], [0.01, 0.03, 0.06]])

# estimate the covariance matrix
cov_matrix = np.cov(returns)

print("Covariance Matrix:")
print(cov_matrix)

"""In this example, we assume that we have historical returns of three assets and store them in a numpy array. We then use the np.corrcoef() function to estimate the constant correlation matrix, which is stored in the corr_matrix variable. The resulting matrix represents the correlation between each pair of assets, assuming a constant value over time.

However, it's important to keep in mind that this approach assumes a constant correlation between assets, which may not hold true in reality, especially in volatile markets. Therefore, this approach should be used with caution and complemented with other techniques for modelling covariance matrices.
"""

import numpy as np

# define the historical returns of three assets
returns = np.array([[0.05, 0.02, 0.01], [0.02, 0.04, 0.03], [0.01, 0.03, 0.06]])

# estimate the constant correlation matrix
corr_matrix = np.corrcoef(returns)

print("Constant Correlation Matrix:")
print(corr_matrix)

"""In this example, we assume that we have historical returns of three assets and store them in a numpy array. We then use PCA to estimate the systematic component of the covariance matrix, which is stored in the sys_component variable. We then estimate the idiosyncratic component of the covariance matrix using univariate variance models, which is stored in the idio_component variable. Finally, we estimate the full covariance matrix using the systematic and idiosyncratic components, which is stored in the cov_matrix variable.

Factor models can capture the interdependence among assets and provide a more accurate estimate of the covariance matrix, especially when the number of assets is large. However, it's important to choose the appropriate number of factors and interpret the results carefully.
"""

import numpy as np
from sklearn.decomposition import PCA

# define the historical returns of three assets
returns = np.array([[0.05, 0.02, 0.01], [0.02, 0.04, 0.03], [0.01, 0.03, 0.06]])

# estimate the systematic component using PCA
pca = PCA(n_components=1)
pca.fit(returns)
sys_component = pca.components_

# estimate the idiosyncratic component using univariate variance models
idio_component = np.diag(np.var(returns, axis=0))

# estimate the covariance matrix
cov_matrix = sys_component @ idio_component @ sys_component.T

print("Factor Model Covariance Matrix:")
print(cov_matrix)

"""In this example, we assume that we have historical returns of three assets and store them in a numpy array. We convert the returns to a pandas DataFrame and then estimate a DCC model using the arch_model function from the arch package. We set the volatility model to DCC, indicating that we want to estimate a dynamic correlation matrix, and set the lag order for the conditional mean and variance models to 1. We then fit the model using the fit method and extract the estimated covariance matrix from the dcc_results object.

DCC models can capture the time-varying nature of the correlation and provide a more accurate estimate of the covariance matrix, especially when the correlation among assets is not constant over time. However, it's important to choose the appropriate lag order and interpret the results carefully.
"""

!pip install arch

import numpy as np
import pandas as pd
from arch import arch_model

# define the historical returns of three assets
returns = np.array([[0.05, 0.02, 0.01], [0.02, 0.04, 0.03], [0.01, 0.03, 0.06]])

# convert returns to pandas DataFrame
returns_df = pd.DataFrame(returns)

# estimate DCC model
dcc_model = arch_model(returns_df, mean='Zero', vol='GARCH', p=1, q=1, o=1, dist='normal', cov_type='DCC')
dcc_results = dcc_model.fit()

# extract the estimated covariance matrix
cov_matrix = dcc_results.conditional_covariance.values[-1]

print("DCC Covariance Matrix:")
print(cov_matrix)

from arch import arch_model

# estimate DCC model
dcc_model = arch_model(returns_df, mean='Zero', vol='GARCH', p=1, q=1, o=1, dist='normal')
dcc_results = dcc_model.fit(disp='off')

"""**Multivariate GARCH models** are widely used in finance and economics to model the time-varying covariance between multiple assets. They are an extension of the univariate GARCH models, where the conditional variance of an asset is modeled as a function of its own past returns and past conditional variances. 

In multivariate GARCH models, the covariance matrix is modeled as a function of the past returns and past covariance matrix. This allows for the modeling of the interdependence among assets and provides a more accurate estimate of the covariance matrix, which is crucial in portfolio optimization, risk management, and asset pricing.

There are several types of multivariate GARCH models, including the constant conditional correlation (CCC) model, the dynamic conditional correlation (DCC) model, and the vector autoregressive GARCH (VAR-GARCH) model, among others. These models differ in how they model the time-varying correlation among assets and how they estimate the covariance matrix.

In practice, multivariate GARCH models are often estimated using maximum likelihood estimation or Bayesian methods, and they require a large sample size to obtain reliable estimates. Despite their computational complexity, multivariate GARCH models are widely used in finance and economics due to their ability to capture the complex interdependence among assets and provide more accurate estimates of risk and expected returns.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from arch import arch_model

# load data
data = pd.read_csv('data.csv', index_col=0)
returns = data.pct_change().dropna()

# define multivariate GARCH model
model = arch_model(returns, p=1, q=1, dist='Normal', mean='constant', vol='GARCH')

# estimate model parameters
result = model.fit()

# print summary of results
print(result.summary())

# plot conditional volatility
fig = result.plot()
plt.show()

import numpy as np
from arch import arch_model

# define a multivariate time series of returns
returns = np.random.normal(size=(1000, 3))
returns = returns.reshape(-1,)


# define and fit the multivariate GARCH model
garch_model = arch_model(returns, mean='Zero', vol='GARCH', p=1, q=1)
garch_results = garch_model.fit()

# print the summary of the fitted model
print(garch_results.summary())

"""**Optimal forecasts and forecast evaluation**



Optimal forecasts are forecasts that are the most accurate and precise predictions of future events, given the available information and the forecasting method being used. In other words, they are the forecasts that minimize the error or deviation between the predicted values and the actual values. 

Forecast evaluation is the process of assessing the accuracy and reliability of forecasts, and it involves comparing the actual outcomes to the predicted values. There are various methods for forecast evaluation, including mean absolute error (MAE), mean squared error (MSE), and root mean squared error (RMSE). These metrics are used to measure the accuracy of a forecasting model by calculating the difference between the predicted values and the actual values.

To achieve optimal forecasts, it is important to use appropriate forecasting techniques and models that take into account the underlying factors that may influence the outcome being forecasted. This requires careful analysis of historical data and identification of the key drivers of the variable being forecasted. Additionally, the use of machine learning algorithms, such as neural networks or decision trees, can also help to improve the accuracy of forecasts.

Forecast evaluation is important because it helps to identify the strengths and weaknesses of a forecasting model, which in turn allows for improvements to be made. It also enables decision-makers to determine the reliability of the forecasts and make better-informed decisions based on the predicted outcomes. Ultimately, the goal of forecast evaluation is to provide insights that lead to improved forecasting accuracy, and thus better decision-making.

Sure! Here are some Python examples for optimal forecasts and forecast evaluation:

"""

import pandas as pd
from statsmodels.tsa.arima.model import ARIMA

# Load data
data = pd.read_csv("data.csv")

# Fit ARIMA model
model = ARIMA(data, order=(2, 1, 0))
model_fit = model.fit()

# Forecast next 10 periods
forecast = model_fit.forecast(steps=10)[0]

# Print forecasted values
print(forecast)

import pandas as pd
from statsmodels.tsa.arima.model import ARIMA

# Create data
data = pd.DataFrame({'value': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]})

# Fit ARIMA model
model = ARIMA(data, order=(2, 1, 0))
model_fit = model.fit()

# Forecast next 10 periods
forecast = model_fit.forecast(steps=10)[0]

# Print forecasted values
print(forecast)

import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
from pandas.api import types

# Load data
data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Stock Data/data.csv")

# Convert index to datetime
data['date'] = pd.to_datetime(data['date'])
data.set_index('date', inplace=True)

# Check if index is a valid DatetimeIndex or PeriodIndex
if not (types.is_datetime64_any_dtype(data.index) or types.is_period_dtype(data.index)):
    raise ValueError("Index must be a DatetimeIndex or PeriodIndex")

# Fit ARIMA model
model = ARIMA(data, order=(2, 1, 0))
model_fit = model.fit()

# Forecast next 10 periods
forecast = model_fit.forecast(steps=10)[0]

# Print forecasted values
print(forecast)

import pandas as pd
from statsmodels.tsa.arima.model import ARIMA

# Load data from CSV file
data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Stock Data/data.csv", parse_dates=["date"], index_col="date")

# Set the frequency of the index to daily (D)
data.index = pd.date_range(start=data.index[0], periods=len(data), freq='D')

# Fit ARIMA model with order (2,1,0)
model = ARIMA(data, order=(2, 1, 0))
model_fit = model.fit()

# Forecast next 10 periods
forecast = model_fit.forecast(steps=10)[0]

# Print forecasted values
print(forecast)

"""Forecast involves comparing it to the theoretically optimal forecast. The theoretically optimal forecast is a forecast that minimizes a chosen loss function, such as mean squared error or mean absolute error. This optimal forecast is based on the information available at the time of the forecast and represents the best prediction possible given that information.

When evaluating a forecast, one can compare the actual outcomes to the forecasted values to calculate measures of accuracy such as mean absolute error, mean squared error, or root mean squared error. These measures can be used to determine how well the forecast performed relative to the theoretically optimal forecast.

It is important to note that the theoretically optimal forecast may not always be achievable in practice, as there may be limitations in the available data or model assumptions that prevent it from being realized. Nonetheless, striving to create forecasts that are as close to optimal as possible can help improve the accuracy and usefulness of forecasts in decision-making processes.

Given a loss function L, an optimal forecast is obtained by minimizing 
the conditional expectation of the future loss
**The concept of an optimal forecast** is often based on the concept of loss functions. A loss function L is a function that measures the cost or penalty associated with making a forecast that is different from the actual value. The optimal forecast is then obtained by minimizing the expected value of the loss function over the set of all possible forecasts.

In other words, given a loss function L, the optimal forecast is the one that minimizes the expected value of the loss function over all possible forecasts. This is equivalent to minimizing the conditional expectation of the future loss, where the future loss is the loss that will be incurred if a particular forecast is made and the actual value turns out to be different.

Different types of loss functions can be used depending on the specific problem at hand. For example, in time series forecasting, common loss functions include mean squared error, mean absolute error, or a combination of the two. The optimal forecast is then obtained by minimizing the expected value of the chosen loss function.

Key components
• What is the target variable, Yt+h? Is it a return over some interval of 
time, the level of some index, the volatility of an asset return?
• What is the information set, Ft
? Is it based only on past observations 
of the target variable available at time t, or can we use information 
from other sources?
• The difference in the time subscripts on the target variable and the 
information set defines the forecast horizon, h.
• What is the loss function, L, that will be used to measure the quality 
of the forecast? A loss function maps the realization of the target 
variable and the forecast to a loss or cost.

**Mincer-Zarnowitz (MZ) regression**

Mincer-Zarnowitz (MZ) regression is a method used for evaluating the accuracy of a set of forecasts. It was proposed by Jacob Mincer and Victor Zarnowitz in a 1969 paper titled "The Evaluation of Economic Forecasts". 

The basic idea of the MZ regression is to estimate a linear regression model that relates the forecast errors to the forecasts themselves. Specifically, the MZ regression model takes the form:

εt = α + βFt + ut

where εt is the forecast error at time t, Ft is the forecast made at time t, and ut is an error term. The coefficients α and β are estimated using ordinary least squares regression. 

If the forecasts are unbiased and efficient, then the MZ regression should have an intercept of zero (i.e., α = 0) and a slope of one (i.e., β = 1). Deviations from these values indicate a lack of accuracy in the forecasts. 

The MZ regression can also be used to test for forecast efficiency, which means that the forecasts are unbiased and the forecast errors are uncorrelated with any available information at the time of the forecast. If the forecasts are efficient, then the error term ut in the MZ regression should be uncorrelated with any information set that could have been used to generate the forecasts.
"""

import numpy as np
import pandas as pd
import statsmodels.api as sm

# Load data from CSV file
data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Stock Data/data.csv", parse_dates=["date"], index_col="date")

# Define the dependent variable (actual values) and the independent variable (forecast values)
y = data["actual"]
x = data["forecast"]

# Add a constant to the independent variable
x = sm.add_constant(x)

# Fit the MZ regression model
model = sm.OLS(y, x)
results = model.fit()

# Print the results summary
print(results.summary())

import pandas as pd
import numpy as np
import statsmodels.api as sm

# create a random dataset with actual and forecasted values
actual = np.random.normal(0, 1, size=50)
forecast = np.random.normal(0.2, 0.5, size=50)

# create a dataframe with actual and forecasted values
df = pd.DataFrame({'actual': actual, 'forecast': forecast})

# run the Mincer-Zarnowitz regression
X = sm.add_constant(df['forecast'])
model = sm.OLS(df['actual'], X)
results = model.fit()

# print the results
print(results.summary())

"""There are different methods for** forecasting conditional variance** in Python, including:

GARCH models from the arch package
EGARCH models from the arch package
HAR models from the pmgarch package
SV models from the stochvol package
Bayesian VAR models from the pymc3 package
"""

import pandas as pd
import arch

# Load data from CSV file
data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Stock Data/data.csv", parse_dates=["date"], index_col="date")

# Fit GARCH(1,1) model
model = arch.arch_model(data, p=1, q=1)
model_fit = model.fit()

# Forecast next 10 periods
forecast = model_fit.forecast(horizon=10)

# Print forecasted volatility
print(forecast.variance.tail(10))

import pandas_datareader.data as web

# Download daily stock prices for Apple
aapl = web.DataReader("AAPL", "yahoo", start="2010-01-01")

# Compute daily returns
aapl_returns = aapl['Adj Close'].pct_change()

import pandas_datareader as web
import datetime as dt

# Define the ticker symbol
ticker = "AAPL"

# Set the start and end dates
start_date = dt.datetime(2010, 1, 1)
end_date = dt.datetime(2021, 4, 28)

# Download daily stock prices for Apple
try:
    aapl = web.DataReader("AAPL", "yahoo", start="2010-01-01")
except Exception as e:
    print("An error occurred: ", e)

# Compute daily returns
aapl_returns = aapl['Adj Close'].pct_change().dropna()

"""Value at Risk (VaR) Analysis: VaR is a widely used risk management technique that measures the maximum potential loss of an investment over a given time period. Python can be used to calculate VaR using various statistical methods such as Monte Carlo simulation.

In this example, we first generate a random stock price series using the numpy.random.normal() function. We then calculate the daily stock returns from the stock price series.

We then define the confidence level and time horizon for VaR calculation. We assume a portfolio value of $1,000,000 at the beginning of the time horizon.

We use the numpy.random.normal() function to generate daily returns for the time horizon. We assume that the daily returns are normally distributed with the same mean and standard deviation as the historical stock returns.

We calculate the portfolio value at the end of the time horizon using the daily returns. We then calculate the VaR using the numpy.percentile() function with the appropriate confidence level.

Finally, we print the VaR for the given portfolio and time horizon. Note that this is just a simple example, and in practice, you would need to use more complex models and data to calculate VaR accurately.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Define the stock price and return series
stock_prices = np.random.normal(100, 10, 1000)
stock_returns = np.diff(stock_prices) / stock_prices[:-1]

# Define the confidence level and time horizon for VaR calculation
confidence_level = 0.95
time_horizon = 10

# Calculate the portfolio value at the end of the time horizon
portfolio_value = 1000000 * np.exp(np.cumsum(stock_returns)[-1])

# Calculate the daily returns for the time horizon
daily_returns = np.random.normal(np.mean(stock_returns), np.std(stock_returns), time_horizon)

# Calculate the portfolio value at the end of the time horizon using the daily returns
portfolio_value_daily = portfolio_value * np.exp(np.cumsum(daily_returns))

# Calculate the VaR using the portfolio value distribution
var = np.percentile(portfolio_value_daily - portfolio_value, (1 - confidence_level) * 100)

print('Value at Risk (VaR) at', confidence_level * 100, '% confidence level for a portfolio worth $', portfolio_value, 'over a time horizon of', time_horizon, 'days is $', round(var, 2))

"""In this example, we first generate a random stock price series using the numpy.random.normal() function. We then calculate the daily stock returns from the stock price series.

We then define the portfolio weights and initial portfolio value. We assume a portfolio with four assets and weights of 40%, 30%, 20%, and 10%.

We define two stress scenarios: a 20% market drop and a 20% market rise. We assume that the stock returns are scaled by the scenario factors to calculate the scenario returns.

We then calculate the portfolio value for each stress scenario using the numpy.exp() and numpy.cumsum() functions.

Finally, we plot the stress test results using the matplotlib.pyplot.bar() function. The bar chart shows the portfolio value for each stress scenario. Note that in practice, you would need to use more realistic stress scenarios and data to perform a proper stress test.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Define the stock price and return series
stock_prices = np.random.normal(100, 10, 1000)
stock_returns = np.diff(stock_prices) / stock_prices[:-1]

# Define the portfolio weights and initial portfolio value
portfolio_weights = np.array([0.4, 0.3, 0.2, 0.1])
portfolio_value = 1000000

# Define the stress scenarios
stress_scenarios = np.array([
    [0.8, 0.9, 0.95, 1.0],  # Scenario 1: 20% market drop
    [1.2, 1.1, 1.05, 1.0]   # Scenario 2: 20% market rise
])

# Calculate the portfolio value for each stress scenario
stress_results = []
for i, scenario in enumerate(stress_scenarios):
    scenario_returns = np.sum(stock_returns * scenario, axis=1)
    scenario_value = portfolio_value * np.exp(np.cumsum(portfolio_weights * scenario_returns)[-1])
    stress_results.append(scenario_value)

# Plot the stress test results
fig, ax = plt.subplots()
ax.bar(['Scenario 1', 'Scenario 2'], stress_results)
ax.set_title('Stress Test Results')
ax.set_ylabel('Portfolio Value')
ax.set_ylim([min(stress_results) * 0.9, max(stress_results) * 1.1])
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Define the stock price and return series
stock_prices = np.random.normal(100, 10, 1000)
stock_returns = np.diff(stock_prices) / stock_prices[:-1]

# Define the portfolio weights and initial portfolio value
portfolio_weights = np.array([0.4, 0.3, 0.2, 0.1])
portfolio_value = 1000000

# Define the stress scenarios
stress_scenarios = np.array([
    [0.8, 0.9, 0.95, 1.0],  # Scenario 1: 20% market drop
    [1.2, 1.1, 1.05, 1.0]   # Scenario 2: 20% market rise
])

# Reshape stock_returns array to have the same number of columns as scenario array
stock_returns = np.reshape(stock_returns, (len(stock_returns), -1))

# Calculate the portfolio value for each stress scenario
stress_results = []
for i, scenario in enumerate(stress_scenarios):
    scenario_returns = np.sum(stock_returns * scenario, axis=1)
    scenario_value = portfolio_value * np.exp(np.cumsum(portfolio_weights * scenario_returns)[-1])
    stress_results.append(scenario_value)

# Plot the stress test results
fig, ax = plt.subplots()
ax.bar(['Scenario 1', 'Scenario 2'], stress_results)
ax.set_title('Stress Test Results')
ax.set_ylabel('Portfolio Value')
ax.set_ylim([min(stress_results) * 0.9, max(stress_results) * 1.1])
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Define the stock price and return series
stock_prices = np.random.normal(100, 10, 1000)
stock_returns = np.diff(stock_prices) / stock_prices[:-1]

# Define the portfolio weights and initial portfolio value
portfolio_weights = np.array([0.4, 0.3, 0.2, 0.1])
portfolio_value = 1000000

# Define the stress scenarios
stress_scenarios = np.array([
    [0.8, 0.9, 0.95, 1.0],  # Scenario 1: 20% market drop
    [1.2, 1.1, 1.05, 1.0]   # Scenario 2: 20% market rise
])

# Reshape portfolio_weights array to have the same number of rows as scenario_returns array
portfolio_weights = np.reshape(portfolio_weights, (-1, 1))

# Calculate the portfolio value for each stress scenario
stress_results = []
for i, scenario in enumerate(stress_scenarios):
    scenario_returns = np.sum(stock_returns * scenario, axis=1)
    scenario_value = portfolio_value * np.exp(np.cumsum(portfolio_weights * scenario_returns)[-1])
    stress_results.append(scenario_value)

# Plot the stress test results
fig, ax = plt.subplots()
ax.bar(['Scenario 1', 'Scenario 2'], stress_results)
ax.set_title('Stress Test Results')
ax.set_ylabel('Portfolio Value')
ax.set_ylim([min(stress_results) * 0.9, max(stress_results) * 1.1])
plt.show()

import numpy as np

# Define portfolio weights
portfolio_weights = np.array([0.2, 0.3, 0.5])

# Define portfolio value
portfolio_value = 1000000

# Define stock returns
stock_returns = np.random.normal(0.001, 0.02, size=(999, 3))

# Define stress scenarios
stress_scenarios = np.array([
    [0.10, -0.10, -0.10],
    [-0.10, 0.10, -0.10],
    [-0.10, -0.10, 0.10],
    [0.20, 0.20, -0.40]
]).T  # Transpose the array

# Calculate scenario returns and values
stress_results = []
for i, scenario in enumerate(stress_scenarios):
    scenario_returns = np.sum(stock_returns * scenario, axis=1)
    scenario_value = portfolio_value * np.exp(np.cumsum(portfolio_weights * scenario_returns)[-1])
    stress_results.append(scenario_value)

# Print stress test results
print("Stress test results:")
for i, scenario_value in enumerate(stress_results):
    print(f"Scenario {i+1}: ${scenario_value:.2f}")

import numpy as np

# Define portfolio weights
portfolio_weights = np.array([0.2, 0.3, 0.5])

# Define portfolio value
portfolio_value = 1000000

# Define stock returns
stock_returns = np.random.normal(0.001, 0.02, size=(999, 3))

# Define stress scenarios
stress_scenarios = np.array([
    [0.10, -0.10, -0.10],
    [-0.10, 0.10, -0.10],
    [-0.10, -0.10, 0.10],
    [0.20, 0.20, -0.40]
]).T  # Transpose the array

# Calculate scenario returns and values
stress_results = []
for i, scenario in enumerate(stress_scenarios):
    scenario_returns = np.sum(stock_returns * scenario, axis=1)
    scenario_value = portfolio_value * np.exp(np.cumsum(portfolio_weights * scenario_returns)[-1])
    stress_results.append(scenario_value)

# Print stress test results
print("Stress test results:")
for i, scenario_value in enumerate(stress_results):
    print(f"Scenario {i+1}: ${scenario_value:.2f}")

import numpy as np

# define stock returns
stock_returns = np.random.normal(loc=0.05, scale=0.1, size=(999, 3))

# define portfolio weights
portfolio_weights = np.array([0.4, 0.3, 0.3])

# define initial portfolio value
portfolio_value = 1000000

# define stress scenarios
stress_scenarios = np.array([
    [0.8, 0.9, 1.1, 1.2], # optimistic
    [0.9, 0.95, 1.05, 1.1], # expected
    [1, 1, 1, 1], # normal
    [1.1, 1.05, 0.95, 0.9], # stressed
])

# run stress scenarios
stress_results = []
for i, scenario in enumerate(stress_scenarios):
    scenario = scenario.reshape((1, 4))
    scenario_returns = np.sum(stock_returns * scenario, axis=1)
    scenario_value = portfolio_value * np.exp(np.cumsum(portfolio_weights * scenario_returns)[-1])
    stress_results.append(scenario_value)

print(stress_results)

import numpy as np
from cvxopt import matrix, solvers

# Define asset returns and covariance matrix
returns = np.array([0.05, 0.1, 0.15, 0.2])
covariance = np.array([[0.04, 0.03, 0.02, 0.01],
                       [0.03, 0.09, 0.05, 0.03],
                       [0.02, 0.05, 0.16, 0.07],
                       [0.01, 0.03, 0.07, 0.25]])

# Define the number of assets in the portfolio
n = len(returns)

# Define the target returns for the portfolio
target_returns = np.linspace(0.05, 0.2, 100)

# Define the constraints for the optimization problem
P = matrix(covariance)
q = matrix(np.zeros((n, 1)))
G = matrix(np.vstack((-np.eye(n), np.eye(n))))
h = matrix(np.vstack((np.zeros((n, 1)), 0.1*np.ones((n, 1)))))
A = matrix(np.vstack((np.ones((1, n)), returns)))
b = matrix(np.array([[1], [0.12]]))

# Perform the optimization
portfolios = [solvers.qp(P, q, G, h, A, matrix(np.array([[r], [1]])))['x'] for r in target_returns]

# Extract the portfolio weights and portfolio risks
weights = [np.array(p).reshape(n,) for p in portfolios]
risks = [np.sqrt(np.dot(np.dot(w.T, covariance), w)) for w in weights]

"""An autoregressive (AR) process is a type of time series model in which the current value of the time series is a linear function of its past values, with the coefficients of the past values determined by an autoregressive parameter. An AR(p) process can be written as:

**Yt = φ1*Yt-1 + φ2*Yt-2 + ... + φp*Yt-p + εt **

where Yt is the current value of the time series, Yt-1, Yt-2, ..., Yt-p are its past values, φ1, φ2, ..., φp are the autoregressive parameters, and εt is a white noise error term with mean zero and constant variance.

Autoregressive processes are commonly used in time series analysis and forecasting, as they can capture the dependence of the current value of the time series on its past values. The order p of the AR process indicates how many past values of the time series are used to predict the current value.

In an AR process, the value of the autoregressive parameter φ determines the strength of the dependence between the current value and its past values. If φ is close to 1, then the dependence is strong, and the time series tends to be persistent over time. If φ is close to 0, then the dependence is weak, and the time series tends to fluctuate randomly over time.

One common approach to estimating the parameters of an AR process is to use the method of maximum likelihood, which involves finding the values of φ that maximize the likelihood of observing the given time series data. Once the parameters are estimated, the AR model can be used to make predictions of future values of the time series.

**A moving average (MA) process** is a type of time series model that is used to capture the short-term dependence between consecutive observations of a time series. In an MA(q) process, the current value of the time series is a linear combination of its past q error terms, where q is the order of the MA process. The general form of an MA(q) process is:

**Yt = μ + εt + θ1*εt-1 + θ2*εt-2 + ... + θq*εt-q**

where Yt is the current value of the time series, μ is the mean of the time series, εt is a white noise error term with mean zero and constant variance, and θ1, θ2, ..., θq are the MA parameters.

The MA(q) process is different from the autoregressive (AR) process in that it models the dependence between observations directly, rather than modeling the dependence between the current value and its past values. The order q of the MA process indicates how many past error terms are used to predict the current value.

The MA(q) process can be used in time series analysis and forecasting to capture the short-term patterns in the data. Like the AR process, the parameters of the MA process can be estimated using the method of maximum likelihood, and the model can be used to make predictions of future values of the time series.

It's worth noting that there is also a related model called the autoregressive moving average (ARMA) process, which combines both AR and MA components into a single model. **The ARMA model is a more flexible and powerful model than either the AR or MA model alone, as it can capture both short-term and long-term dependence patterns in the data.**
"""

import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm

# Simulate an AR(1) process with phi = 0.8 and white noise error
ar_model = sm.tsa.ArmaProcess(ar=[1, -0.8], ma=[1])
ar_data = ar_model.generate_sample(nsample=100)

# Simulate an MA(1) process with theta = 0.5 and white noise error
ma_model = sm.tsa.ArmaProcess(ar=[1], ma=[1, 0.5])
ma_data = ma_model.generate_sample(nsample=100)

# Plot the AR and MA time series
fig, ax = plt.subplots(2, 1, figsize=(8, 6))
ax[0].plot(ar_data)
ax[0].set(title='AR(1) process (phi = 0.8)', xlabel='Time', ylabel='Yt')
ax[1].plot(ma_data)
ax[1].set(title='MA(1) process (theta = 0.5)', xlabel='Time', ylabel='Yt')
plt.tight_layout()
plt.show()

"""An **ARMA (Autoregressive Moving Average)** process is a stochastic process that combines both autoregressive and moving average components to model a time series. 

The autoregressive component of an ARMA process models the dependence of the current value on the past values of the time series, while the moving average component models the dependence on past errors.

An ARMA(p,q) process can be represented by the following equation:

**Y_t = c + phi_1*Y_{t-1} + ... + phi_p*Y_{t-p} + e_t + theta_1*e_{t-1} + ... + theta_q*e_{t-q}**

where Y_t is the value of the time series at time t, c is a constant, phi_1, ..., phi_p are the autoregressive coefficients of the past values, e_t is a white noise error term with mean zero and variance sigma^2, and theta_1, ..., theta_q are the moving average coefficients of the past errors.

The ARMA process assumes that the time series is stationary, meaning that the mean and variance of the process remain constant over time. The parameters of the ARMA process can be estimated using methods such as maximum likelihood estimation or least squares estimation. The fitted ARMA model can then be used to make predictions and generate forecasts for future values of the time series.
"""

import statsmodels.api as sm

# load the time series data
y = [0.5, 0.7, 0.1, 0.3, 0.9, 0.2, 0.6, 0.4, 0.8, 0.5]

# fit the ARMA(1,1) model
model = sm.tsa.ARMA(y, order=(1,1)).fit()

# print the model summary
print(model.summary())

# make a forecast for the next 5 time periods
forecast = model.forecast(steps=5)

# print the forecast values
print(forecast)

import statsmodels.api as sm

# load the time series data
y = [0.5, 0.7, 0.1, 0.3, 0.9, 0.2, 0.6, 0.4, 0.8, 0.5]

# fit the ARMA(1,1) model
model = sm.tsa.SARIMAX(y, order=(1,0,1)).fit()

# print the model summary
print(model.summary())

# make a forecast for the next 5 time periods
forecast = model.forecast(steps=5)

# print the forecast values
print(forecast)

"""

When combining three forecasts, there are several methods that can be used to optimize the accuracy of the resulting forecast. Here are some options:

1.** Simple averaging**: This method involves taking the average of the three forecasts. This can be a good option when the three forecasts are equally reliable and have similar accuracy. However, it may not work as well when there is a large discrepancy between the forecasts.

2. **Weighted averaging**: This method assigns different weights to each of the three forecasts based on their perceived accuracy. For example, if one of the forecasts is considered more reliable than the others, it can be given a higher weight. The weights can be based on expert judgement or statistical methods.

3. **Ensemble methods**: This approach involves combining the three forecasts using statistical methods, such as regression or machine learning algorithms. Ensemble methods can be particularly effective when the three forecasts have different strengths and weaknesses. For example, one forecast may be better at predicting short-term trends while another may be better at capturing long-term patterns.

4. **Bayesian model averaging**: This method uses Bayesian statistics to combine the three forecasts. It involves calculating the posterior probability of each forecast given the available data and then combining them using Bayes' theorem. This approach can be particularly effective when there is uncertainty about the accuracy of each forecast.

Ultimately, the choice of method will depend on the specific situation and the characteristics of the three forecasts. It may be useful to experiment with different methods and compare their accuracy using historical data or other metrics."""

def simple_average(forecasts):
    return sum(forecasts) / len(forecasts)

# Example usage:
forecasts = [10, 15, 20]
result = simple_average(forecasts)
print(result)  # Output: 15.0

def weighted_average(forecasts, weights):
    assert len(forecasts) == len(weights), "Forecasts and weights must have the same length"
    total_weight = sum(weights)
    weighted_forecasts = [f * w for f, w in zip(forecasts, weights)]
    return sum(weighted_forecasts) / total_weight

# Example usage:
forecasts = [10, 15, 20]
weights = [0.3, 0.4, 0.3]
result = weighted_average(forecasts, weights)
print(result)  # Output: 15.0

from sklearn.linear_model import LinearRegression
import numpy as np

def ensemble_method(forecasts):
    n_samples, n_forecasts = np.shape(forecasts)
    X = np.tile(np.arange(n_forecasts), (n_samples, 1))
    X = X.reshape((n_samples*n_forecasts, 1))
    y = np.array(forecasts).flatten()
    model = LinearRegression()
    model.fit(X, y)
    return model.predict(np.arange(n_forecasts).reshape((-1,1)))

# Example usage:
forecasts = [[10, 12, 14], [9, 15, 21], [8, 18, 28]]
result = ensemble_method(forecasts)
print(result)  # Output: [ 9. 15. 21.]

import numpy as np
from scipy.stats import norm

def bayesian_model_averaging(forecasts):
    weights = [1.0 / len(forecasts) for _ in range(len(forecasts))]
    prior_mean = np.mean(forecasts)
    prior_std = np.std(forecasts)
    likelihoods = [norm.pdf(forecasts[i], loc=prior_mean, scale=prior_std) for i in range(len(forecasts))]
    posterior_unnormalized = [weights[i] * likelihoods[i] for i in range(len(forecasts))]
    posterior = [pu / sum(posterior_unnormalized) for pu in posterior_unnormalized]
    return sum([forecasts[i] * posterior[i] for i in range(len(forecasts))])

# Example usage:
forecasts = [10, 15, 20]
result = bayesian_model_averaging(forecasts)
print(result)  # Output: 15.0

import numpy as np
import statsmodels.api as sm
from scipy.stats import t

# Sample data
y_n = np.array([1, 2, 3, 4, 5])
y_b = np.array([2, 3, 4, 5, 6])
u = np.array([0.1, 0.2, -0.1, -0.2, 0.3])

# Regression
X = np.column_stack([y_n**2, y_b])
X = sm.add_constant(X)
model = sm.OLS(y_n + u, X)
results = model.fit()

# Hypothesis testing
tA = (results.params[1] - 1) / results.bse[1]
tB = results.params[2] / results.bse[2]
pA = 2 * (1 - t.cdf(abs(tA), results.df_resid))
pB = 2 * (1 - t.cdf(abs(tB), results.df_resid))
alpha = 0.05
if pA < alpha:
    print("Reject H0^A: yt+1^n does not perfectly encompass yt^b.")
else:
    print("Fail to reject H0^A.")
if pB < alpha:
    print("Reject H0^B: yt^b does not perfectly encompass yt+1^n.")
else:
    print("Fail to reject H0^B.")